{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facies classification using Machine Learning #\n",
    "## LA Team Submission 6 ## \n",
    "### _[Lukas Mosser](https://at.linkedin.com/in/lukas-mosser-9948b32b/en), [Alfredo De la Fuente](https://pe.linkedin.com/in/alfredodelafuenteb)_ ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach for solving the facies classfication problem ( https://github.com/seg/2016-ml-contest. ) we will explore the following statregies:\n",
    "- Features Exploration: based on [Paolo Bestagini's work](https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try02.ipynb), we will consider imputation, normalization and augmentation routines for the initial features.\n",
    "- Model tuning: we use TPOT to come up with a good enough pipeline, and then tune the hyperparameters of the model obtained using HYPEROPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pandas in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): python-dateutil in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pytz>=2011k in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy>=1.7.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six>=1.5 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from python-dateutil->pandas)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): tpot in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): deap in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): update-checker in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): tqdm in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages (from tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests>=2.3.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from update-checker->tpot)\n",
      "Requirement already satisfied (use --upgrade to upgrade): hyperopt in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): networkx in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): nose in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): future in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): pymongo in /home/alfredo/anaconda2/lib/python2.7/site-packages (from hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): decorator>=3.4.0 in /home/alfredo/anaconda2/lib/python2.7/site-packages (from networkx->hyperopt)\n",
      "Requirement already satisfied (use --upgrade to upgrade): xgboost in /home/alfredo/anaconda2/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): numpy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scipy in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n",
      "Requirement already satisfied (use --upgrade to upgrade): scikit-learn in /home/alfredo/anaconda2/lib/python2.7/site-packages (from xgboost)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "You are using pip version 8.1.2, however version 9.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "pip install pandas\n",
    "pip install scikit-learn\n",
    "pip install tpot\n",
    "pip install hyperopt\n",
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "import numpy as np\n",
    "#%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold , StratifiedKFold\n",
    "from classification_utilities import display_cm, display_adj_cm\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn import preprocessing, impute\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from scipy.signal import medfilt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing \n",
    "\n",
    "We procceed to run [Paolo Bestagini's routine](https://github.com/seg/2016-ml-contest/blob/master/ispl/facies_classification_try02.ipynb) to include a small window of values to acount for the spatial component in the log analysis, as well as the gradient information with respect to depth. This will be our prepared training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load Data\n",
    "data = pd.read_csv('../facies_vectors.csv')\n",
    "\n",
    "# Parameters\n",
    "feature_names = ['GR', 'ILD_log10', 'DeltaPHI', 'PHIND', 'PE', 'NM_M', 'RELPOS']\n",
    "facies_names = ['SS', 'CSiS', 'FSiS', 'SiSh', 'MS', 'WS', 'D', 'PS', 'BS']\n",
    "facies_colors = ['#F4D03F', '#F5B041','#DC7633','#6E2C00', '#1B4F72','#2E86C1', '#AED6F1', '#A569BD', '#196F3D']\n",
    "\n",
    "# Store features and labels\n",
    "X = data[feature_names].values \n",
    "y = data['Facies'].values \n",
    "\n",
    "# Store well labels and depths\n",
    "well = data['Well Name'].values\n",
    "depth = data['Depth'].values\n",
    "\n",
    "# Fill 'PE' missing values with mean\n",
    "imp = impute.SimpleImputer(strategy='mean')   #Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "imp.fit(X)\n",
    "X = imp.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature windows concatenation function\n",
    "def augment_features_window(X, N_neig):\n",
    "    \n",
    "    # Parameters\n",
    "    N_row = X.shape[0]\n",
    "    N_feat = X.shape[1]\n",
    "\n",
    "    # Zero padding\n",
    "    X = np.vstack((np.zeros((N_neig, N_feat)), X, (np.zeros((N_neig, N_feat)))))\n",
    "\n",
    "    # Loop over windows\n",
    "    X_aug = np.zeros((N_row, N_feat*(2*N_neig+1)))\n",
    "    for r in np.arange(N_row)+N_neig:\n",
    "        this_row = []\n",
    "        for c in np.arange(-N_neig,N_neig+1):\n",
    "            this_row = np.hstack((this_row, X[r+c]))\n",
    "        X_aug[r-N_neig] = this_row\n",
    "\n",
    "    return X_aug\n",
    "\n",
    "\n",
    "# Feature gradient computation function\n",
    "def augment_features_gradient(X, depth):\n",
    "    \n",
    "    # Compute features gradient\n",
    "    d_diff = np.diff(depth).reshape((-1, 1))\n",
    "    d_diff[d_diff==0] = 0.001\n",
    "    X_diff = np.diff(X, axis=0)\n",
    "    X_grad = X_diff / d_diff\n",
    "        \n",
    "    # Compensate for last missing value\n",
    "    X_grad = np.concatenate((X_grad, np.zeros((1, X_grad.shape[1]))))\n",
    "    \n",
    "    return X_grad\n",
    "\n",
    "\n",
    "# Feature augmentation function\n",
    "def augment_features(X, well, depth, N_neig=1):\n",
    "    \n",
    "    # Augment features\n",
    "    X_aug = np.zeros((X.shape[0], X.shape[1]*(N_neig*2+2)))\n",
    "    for w in np.unique(well):\n",
    "        w_idx = np.where(well == w)[0]\n",
    "        X_aug_win = augment_features_window(X[w_idx, :], N_neig)\n",
    "        X_aug_grad = augment_features_gradient(X[w_idx, :], depth[w_idx])\n",
    "        X_aug[w_idx, :] = np.concatenate((X_aug_win, X_aug_grad), axis=1)\n",
    "    \n",
    "    # Find padded rows\n",
    "    padded_rows = np.unique(np.where(X_aug[:, 0:7] == np.zeros((1, 7)))[0])\n",
    "    \n",
    "    return X_aug, padded_rows\n",
    "\n",
    "X_aug, padded_rows = augment_features(X, well, depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize model selection methods\n",
    "lpgo = LeavePGroupsOut(2)\n",
    "\n",
    "# Generate splits\n",
    "split_list = []\n",
    "for train, val in lpgo.split(X, y, groups=data['Well Name']):\n",
    "    hist_tr = np.histogram(y[train], bins=np.arange(len(facies_names)+1)+.5)\n",
    "    hist_val = np.histogram(y[val], bins=np.arange(len(facies_names)+1)+.5)\n",
    "    if np.all(hist_tr[0] != 0) & np.all(hist_val[0] != 0):\n",
    "        split_list.append({'train':train, 'val':val})\n",
    "    \n",
    "        \n",
    "def preprocess():\n",
    "    \n",
    "    # Preprocess data to use in model\n",
    "    X_train_aux = []\n",
    "    X_test_aux = []\n",
    "    y_train_aux = []\n",
    "    y_test_aux = []\n",
    "    \n",
    "    # For each data split\n",
    "    for split in split_list:\n",
    "        # Remove padded rows\n",
    "        split_train_no_pad = np.setdiff1d(split['train'], padded_rows)\n",
    "\n",
    "        # Select training and validation data from current split\n",
    "        X_tr = X_aug[split_train_no_pad, :]\n",
    "        X_v = X_aug[split['val'], :]\n",
    "        y_tr = y[split_train_no_pad]\n",
    "        y_v = y[split['val']]\n",
    "\n",
    "        # Select well labels for validation data\n",
    "        well_v = well[split['val']]\n",
    "\n",
    "        # Feature normalization\n",
    "        scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "        X_tr = scaler.transform(X_tr)\n",
    "        X_v = scaler.transform(X_v)\n",
    "\n",
    "        X_train_aux.append( X_tr )\n",
    "        X_test_aux.append( X_v )\n",
    "        y_train_aux.append( y_tr )\n",
    "        y_test_aux.append (  y_v )\n",
    "\n",
    "        X_train = np.concatenate( X_train_aux )\n",
    "        X_test = np.concatenate ( X_test_aux )\n",
    "        y_train = np.concatenate ( y_train_aux )\n",
    "        y_test = np.concatenate ( y_test_aux )\n",
    "    \n",
    "    return X_train , X_test , y_train , y_test \n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess()\n",
    "y_train = y_train - 1 \n",
    "y_test = y_test - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "In this section we will run a Cross Validation routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import  XGBClassifier\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SEED = 314159265\n",
    "VALID_SIZE = 0.2\n",
    "TARGET = 'outcome'\n",
    "\n",
    "# Scoring and optimization functions\n",
    "\n",
    "def score(params):\n",
    "    print(\"Training with params: \")\n",
    "    print(params)\n",
    "    #clf = xgb.XGBClassifier(**params) \n",
    "    #clf.fit(X_train, y_train)\n",
    "    #y_predictions = clf.predict(X_test)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(X_test, label=y_test)\n",
    "    watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    gbm_model = xgb.train(params, dtrain, num_round,\n",
    "                          evals=watchlist,\n",
    "                          verbose_eval=True)\n",
    "    if hasattr(gbm_model, 'best_iteration') and gbm_model.best_iteration is not None:\n",
    "        best_iteration = gbm_model.best_iteration\n",
    "    else:\n",
    "        best_iteration = gbm_model.num_boosted_rounds()\n",
    "    \n",
    "    print(f\"Best iteration: {best_iteration}\")\n",
    "    y_predictions = gbm_model.predict(dvalid,\n",
    "                                    iteration_range=(0,best_iteration))\n",
    "    \n",
    "    score = f1_score (y_test, y_predictions , average ='micro')\n",
    "    print(\"\\tScore {0}\\n\\n\".format(score))\n",
    "    loss = 1 - score\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(random_state=SEED):\n",
    "    space = {\n",
    "        'n_estimators': hp.quniform('n_estimators', 100, 150, 1),\n",
    "        'eta': hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "        'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "        'min_child_weight': hp.quniform('min_child_weight', 1, 6, 1),\n",
    "        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "        'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'objective': 'multi:softmax',\n",
    "        'nthread': 4,\n",
    "        'booster': 'gbtree',\n",
    "        'tree_method': 'exact',\n",
    "        'silent': 1,\n",
    "        'num_class' : 9,\n",
    "        'seed': random_state\n",
    "    }\n",
    "    # Use the fmin function from Hyperopt to find the best hyperparameters\n",
    "    best = fmin(score, space, algo=tpe.suggest,  max_evals=5)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params:                                \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.75, 'eta': 0.47500000000000003, 'eval_metric': 'mlogloss', 'gamma': 0.7000000000000001, 'max_depth': 1, 'min_child_weight': 3.0, 'n_estimators': 128.0, 'nthread': 4, 'num_class': 9, 'objective': 'multi:softmax', 'seed': 314159265, 'silent': 1, 'subsample': 0.65, 'tree_method': 'exact'}\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jan67814\\AppData\\Local\\miniforge3\\envs\\env_dl_ml\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:34:54] WARNING: D:\\bld\\xgboost-split_1738395203399\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:1.85689\ttrain-mlogloss:1.84919     \n",
      "[1]\teval-mlogloss:1.71032\ttrain-mlogloss:1.69242     \n",
      "[2]\teval-mlogloss:1.60690\ttrain-mlogloss:1.58130     \n",
      "[3]\teval-mlogloss:1.53832\ttrain-mlogloss:1.50558     \n",
      "[4]\teval-mlogloss:1.48368\ttrain-mlogloss:1.44821     \n",
      "[5]\teval-mlogloss:1.44155\ttrain-mlogloss:1.40150     \n",
      "[6]\teval-mlogloss:1.40309\ttrain-mlogloss:1.36350     \n",
      "[7]\teval-mlogloss:1.37187\ttrain-mlogloss:1.33125     \n",
      "[8]\teval-mlogloss:1.34464\ttrain-mlogloss:1.30272     \n",
      "[9]\teval-mlogloss:1.31781\ttrain-mlogloss:1.27850     \n",
      "[10]\teval-mlogloss:1.29291\ttrain-mlogloss:1.25566    \n",
      "[11]\teval-mlogloss:1.27298\ttrain-mlogloss:1.23539    \n",
      "[12]\teval-mlogloss:1.25282\ttrain-mlogloss:1.21562    \n",
      "[13]\teval-mlogloss:1.23582\ttrain-mlogloss:1.19838    \n",
      "[14]\teval-mlogloss:1.22125\ttrain-mlogloss:1.18275    \n",
      "[15]\teval-mlogloss:1.20750\ttrain-mlogloss:1.16811    \n",
      "[16]\teval-mlogloss:1.19239\ttrain-mlogloss:1.15402    \n",
      "[17]\teval-mlogloss:1.17968\ttrain-mlogloss:1.14080    \n",
      "[18]\teval-mlogloss:1.16844\ttrain-mlogloss:1.12921    \n",
      "[19]\teval-mlogloss:1.15366\ttrain-mlogloss:1.11805    \n",
      "[20]\teval-mlogloss:1.14456\ttrain-mlogloss:1.10782    \n",
      "[21]\teval-mlogloss:1.13371\ttrain-mlogloss:1.09786    \n",
      "[22]\teval-mlogloss:1.12487\ttrain-mlogloss:1.08862    \n",
      "[23]\teval-mlogloss:1.11616\ttrain-mlogloss:1.07992    \n",
      "[24]\teval-mlogloss:1.10675\ttrain-mlogloss:1.07177    \n",
      "[25]\teval-mlogloss:1.09939\ttrain-mlogloss:1.06379    \n",
      "[26]\teval-mlogloss:1.09217\ttrain-mlogloss:1.05614    \n",
      "[27]\teval-mlogloss:1.08516\ttrain-mlogloss:1.04888    \n",
      "[28]\teval-mlogloss:1.07786\ttrain-mlogloss:1.04220    \n",
      "[29]\teval-mlogloss:1.07113\ttrain-mlogloss:1.03578    \n",
      "[30]\teval-mlogloss:1.06611\ttrain-mlogloss:1.02953    \n",
      "[31]\teval-mlogloss:1.06016\ttrain-mlogloss:1.02368    \n",
      "[32]\teval-mlogloss:1.05499\ttrain-mlogloss:1.01819    \n",
      "[33]\teval-mlogloss:1.04961\ttrain-mlogloss:1.01301    \n",
      "[34]\teval-mlogloss:1.04523\ttrain-mlogloss:1.00800    \n",
      "[35]\teval-mlogloss:1.04035\ttrain-mlogloss:1.00315    \n",
      "[36]\teval-mlogloss:1.03553\ttrain-mlogloss:0.99827    \n",
      "[37]\teval-mlogloss:1.03130\ttrain-mlogloss:0.99360    \n",
      "[38]\teval-mlogloss:1.02580\ttrain-mlogloss:0.98905    \n",
      "[39]\teval-mlogloss:1.02148\ttrain-mlogloss:0.98462    \n",
      "[40]\teval-mlogloss:1.01699\ttrain-mlogloss:0.98055    \n",
      "[41]\teval-mlogloss:1.01290\ttrain-mlogloss:0.97654    \n",
      "[42]\teval-mlogloss:1.00862\ttrain-mlogloss:0.97267    \n",
      "[43]\teval-mlogloss:1.00436\ttrain-mlogloss:0.96886    \n",
      "[44]\teval-mlogloss:1.00031\ttrain-mlogloss:0.96500    \n",
      "[45]\teval-mlogloss:0.99675\ttrain-mlogloss:0.96119    \n",
      "[46]\teval-mlogloss:0.99280\ttrain-mlogloss:0.95782    \n",
      "[47]\teval-mlogloss:0.98932\ttrain-mlogloss:0.95462    \n",
      "[48]\teval-mlogloss:0.98672\ttrain-mlogloss:0.95132    \n",
      "[49]\teval-mlogloss:0.98286\ttrain-mlogloss:0.94806    \n",
      "[50]\teval-mlogloss:0.97952\ttrain-mlogloss:0.94489    \n",
      "[51]\teval-mlogloss:0.97642\ttrain-mlogloss:0.94192    \n",
      "[52]\teval-mlogloss:0.97365\ttrain-mlogloss:0.93895    \n",
      "[53]\teval-mlogloss:0.97113\ttrain-mlogloss:0.93584    \n",
      "[54]\teval-mlogloss:0.96882\ttrain-mlogloss:0.93292    \n",
      "[55]\teval-mlogloss:0.96650\ttrain-mlogloss:0.93012    \n",
      "[56]\teval-mlogloss:0.96404\ttrain-mlogloss:0.92734    \n",
      "[57]\teval-mlogloss:0.96212\ttrain-mlogloss:0.92475    \n",
      "[58]\teval-mlogloss:0.95945\ttrain-mlogloss:0.92216    \n",
      "[59]\teval-mlogloss:0.95730\ttrain-mlogloss:0.91961    \n",
      "[60]\teval-mlogloss:0.95433\ttrain-mlogloss:0.91716    \n",
      "[61]\teval-mlogloss:0.95252\ttrain-mlogloss:0.91479    \n",
      "[62]\teval-mlogloss:0.95058\ttrain-mlogloss:0.91249    \n",
      "[63]\teval-mlogloss:0.94790\ttrain-mlogloss:0.91029    \n",
      "[64]\teval-mlogloss:0.94533\ttrain-mlogloss:0.90800    \n",
      "[65]\teval-mlogloss:0.94336\ttrain-mlogloss:0.90572    \n",
      "[66]\teval-mlogloss:0.94138\ttrain-mlogloss:0.90339    \n",
      "[67]\teval-mlogloss:0.93937\ttrain-mlogloss:0.90121    \n",
      "[68]\teval-mlogloss:0.93751\ttrain-mlogloss:0.89899    \n",
      "[69]\teval-mlogloss:0.93491\ttrain-mlogloss:0.89688    \n",
      "[70]\teval-mlogloss:0.93283\ttrain-mlogloss:0.89483    \n",
      "[71]\teval-mlogloss:0.93030\ttrain-mlogloss:0.89277    \n",
      "[72]\teval-mlogloss:0.92925\ttrain-mlogloss:0.89065    \n",
      "[73]\teval-mlogloss:0.92689\ttrain-mlogloss:0.88863    \n",
      "[74]\teval-mlogloss:0.92481\ttrain-mlogloss:0.88675    \n",
      "[75]\teval-mlogloss:0.92299\ttrain-mlogloss:0.88483    \n",
      "[76]\teval-mlogloss:0.92151\ttrain-mlogloss:0.88300    \n",
      "[77]\teval-mlogloss:0.91945\ttrain-mlogloss:0.88108    \n",
      "[78]\teval-mlogloss:0.91725\ttrain-mlogloss:0.87932    \n",
      "[79]\teval-mlogloss:0.91549\ttrain-mlogloss:0.87756    \n",
      "[80]\teval-mlogloss:0.91431\ttrain-mlogloss:0.87581    \n",
      "[81]\teval-mlogloss:0.91272\ttrain-mlogloss:0.87415    \n",
      "[82]\teval-mlogloss:0.91165\ttrain-mlogloss:0.87252    \n",
      "[83]\teval-mlogloss:0.91024\ttrain-mlogloss:0.87089    \n",
      "[84]\teval-mlogloss:0.90807\ttrain-mlogloss:0.86913    \n",
      "[85]\teval-mlogloss:0.90641\ttrain-mlogloss:0.86746    \n",
      "[86]\teval-mlogloss:0.90484\ttrain-mlogloss:0.86584    \n",
      "[87]\teval-mlogloss:0.90300\ttrain-mlogloss:0.86416    \n",
      "[88]\teval-mlogloss:0.90137\ttrain-mlogloss:0.86253    \n",
      "[89]\teval-mlogloss:0.89960\ttrain-mlogloss:0.86094    \n",
      "[90]\teval-mlogloss:0.89855\ttrain-mlogloss:0.85946    \n",
      "[91]\teval-mlogloss:0.89730\ttrain-mlogloss:0.85787    \n",
      "[92]\teval-mlogloss:0.89626\ttrain-mlogloss:0.85633    \n",
      "[93]\teval-mlogloss:0.89495\ttrain-mlogloss:0.85483    \n",
      "[94]\teval-mlogloss:0.89354\ttrain-mlogloss:0.85340    \n",
      "[95]\teval-mlogloss:0.89262\ttrain-mlogloss:0.85194    \n",
      "[96]\teval-mlogloss:0.89126\ttrain-mlogloss:0.85047    \n",
      "[97]\teval-mlogloss:0.88990\ttrain-mlogloss:0.84907    \n",
      "[98]\teval-mlogloss:0.88872\ttrain-mlogloss:0.84772    \n",
      "[99]\teval-mlogloss:0.88702\ttrain-mlogloss:0.84638    \n",
      "[100]\teval-mlogloss:0.88578\ttrain-mlogloss:0.84496   \n",
      "[101]\teval-mlogloss:0.88462\ttrain-mlogloss:0.84364   \n",
      "[102]\teval-mlogloss:0.88295\ttrain-mlogloss:0.84232   \n",
      "[103]\teval-mlogloss:0.88123\ttrain-mlogloss:0.84103   \n",
      "[104]\teval-mlogloss:0.87980\ttrain-mlogloss:0.83972   \n",
      "[105]\teval-mlogloss:0.87895\ttrain-mlogloss:0.83843   \n",
      "[106]\teval-mlogloss:0.87761\ttrain-mlogloss:0.83712   \n",
      "[107]\teval-mlogloss:0.87641\ttrain-mlogloss:0.83582   \n",
      "[108]\teval-mlogloss:0.87488\ttrain-mlogloss:0.83456   \n",
      "[109]\teval-mlogloss:0.87349\ttrain-mlogloss:0.83331   \n",
      "[110]\teval-mlogloss:0.87230\ttrain-mlogloss:0.83207   \n",
      "[111]\teval-mlogloss:0.87121\ttrain-mlogloss:0.83091   \n",
      "[112]\teval-mlogloss:0.87034\ttrain-mlogloss:0.82976   \n",
      "[113]\teval-mlogloss:0.86914\ttrain-mlogloss:0.82853   \n",
      "[114]\teval-mlogloss:0.86788\ttrain-mlogloss:0.82735   \n",
      "[115]\teval-mlogloss:0.86673\ttrain-mlogloss:0.82618   \n",
      "[116]\teval-mlogloss:0.86535\ttrain-mlogloss:0.82501   \n",
      "[117]\teval-mlogloss:0.86427\ttrain-mlogloss:0.82385   \n",
      "[118]\teval-mlogloss:0.86316\ttrain-mlogloss:0.82279   \n",
      "[119]\teval-mlogloss:0.86196\ttrain-mlogloss:0.82170   \n",
      "[120]\teval-mlogloss:0.86035\ttrain-mlogloss:0.82059   \n",
      "[121]\teval-mlogloss:0.85932\ttrain-mlogloss:0.81953   \n",
      "[122]\teval-mlogloss:0.85792\ttrain-mlogloss:0.81843   \n",
      "[123]\teval-mlogloss:0.85671\ttrain-mlogloss:0.81729   \n",
      "[124]\teval-mlogloss:0.85613\ttrain-mlogloss:0.81624   \n",
      "[125]\teval-mlogloss:0.85517\ttrain-mlogloss:0.81516   \n",
      "[126]\teval-mlogloss:0.85432\ttrain-mlogloss:0.81414   \n",
      "[127]\teval-mlogloss:0.85339\ttrain-mlogloss:0.81313   \n",
      "Best iteration: 128                                  \n",
      "\tScore 0.6789908294949998                            \n",
      "\n",
      "\n",
      "Training with params:                                                          \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.7000000000000001, 'eta': 0.275, 'eval_metric': 'mlogloss', 'gamma': 0.9500000000000001, 'max_depth': 6, 'min_child_weight': 2.0, 'n_estimators': 137.0, 'nthread': 4, 'num_class': 9, 'objective': 'multi:softmax', 'seed': 314159265, 'silent': 1, 'subsample': 0.75, 'tree_method': 'exact'}\n",
      " 20%|██        | 1/5 [00:27<01:50, 27.51s/trial, best loss: 0.3210091705050002]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jan67814\\AppData\\Local\\miniforge3\\envs\\env_dl_ml\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:35:21] WARNING: D:\\bld\\xgboost-split_1738395203399\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:1.65931\ttrain-mlogloss:1.64361                               \n",
      "[1]\teval-mlogloss:1.39705\ttrain-mlogloss:1.37359                               \n",
      "[2]\teval-mlogloss:1.20732\ttrain-mlogloss:1.17828                               \n",
      "[3]\teval-mlogloss:1.06725\ttrain-mlogloss:1.03522                               \n",
      "[4]\teval-mlogloss:0.96116\ttrain-mlogloss:0.92717                               \n",
      "[5]\teval-mlogloss:0.87696\ttrain-mlogloss:0.84164                               \n",
      "[6]\teval-mlogloss:0.80126\ttrain-mlogloss:0.76428                               \n",
      "[7]\teval-mlogloss:0.74627\ttrain-mlogloss:0.70888                               \n",
      "[8]\teval-mlogloss:0.69670\ttrain-mlogloss:0.65898                               \n",
      "[9]\teval-mlogloss:0.65081\ttrain-mlogloss:0.61229                               \n",
      "[10]\teval-mlogloss:0.61519\ttrain-mlogloss:0.57655                              \n",
      "[11]\teval-mlogloss:0.57982\ttrain-mlogloss:0.54165                              \n",
      "[12]\teval-mlogloss:0.55293\ttrain-mlogloss:0.51462                              \n",
      "[13]\teval-mlogloss:0.52649\ttrain-mlogloss:0.48766                              \n",
      "[14]\teval-mlogloss:0.50542\ttrain-mlogloss:0.46713                              \n",
      "[15]\teval-mlogloss:0.48356\ttrain-mlogloss:0.44571                              \n",
      "[16]\teval-mlogloss:0.46469\ttrain-mlogloss:0.42741                              \n",
      "[17]\teval-mlogloss:0.44808\ttrain-mlogloss:0.41030                              \n",
      "[18]\teval-mlogloss:0.43028\ttrain-mlogloss:0.39185                              \n",
      "[19]\teval-mlogloss:0.41589\ttrain-mlogloss:0.37764                              \n",
      "[20]\teval-mlogloss:0.40120\ttrain-mlogloss:0.36283                              \n",
      "[21]\teval-mlogloss:0.38900\ttrain-mlogloss:0.35031                              \n",
      "[22]\teval-mlogloss:0.37752\ttrain-mlogloss:0.33917                              \n",
      "[23]\teval-mlogloss:0.36804\ttrain-mlogloss:0.32970                              \n",
      "[24]\teval-mlogloss:0.35864\ttrain-mlogloss:0.31984                              \n",
      "[25]\teval-mlogloss:0.34825\ttrain-mlogloss:0.30977                              \n",
      "[26]\teval-mlogloss:0.33726\ttrain-mlogloss:0.29828                              \n",
      "[27]\teval-mlogloss:0.32795\ttrain-mlogloss:0.28923                              \n",
      "[28]\teval-mlogloss:0.31625\ttrain-mlogloss:0.27780                              \n",
      "[29]\teval-mlogloss:0.30605\ttrain-mlogloss:0.26753                              \n",
      "[30]\teval-mlogloss:0.29534\ttrain-mlogloss:0.25699                              \n",
      "[31]\teval-mlogloss:0.28584\ttrain-mlogloss:0.24773                              \n",
      "[32]\teval-mlogloss:0.27751\ttrain-mlogloss:0.23973                              \n",
      "[33]\teval-mlogloss:0.26717\ttrain-mlogloss:0.22906                              \n",
      "[34]\teval-mlogloss:0.25813\ttrain-mlogloss:0.21998                              \n",
      "[35]\teval-mlogloss:0.25055\ttrain-mlogloss:0.21184                              \n",
      "[36]\teval-mlogloss:0.24332\ttrain-mlogloss:0.20488                              \n",
      "[37]\teval-mlogloss:0.23650\ttrain-mlogloss:0.19780                              \n",
      "[38]\teval-mlogloss:0.22661\ttrain-mlogloss:0.18804                              \n",
      "[39]\teval-mlogloss:0.21939\ttrain-mlogloss:0.18050                              \n",
      "[40]\teval-mlogloss:0.21082\ttrain-mlogloss:0.17170                              \n",
      "[41]\teval-mlogloss:0.20478\ttrain-mlogloss:0.16579                              \n",
      "[42]\teval-mlogloss:0.19738\ttrain-mlogloss:0.15851                              \n",
      "[43]\teval-mlogloss:0.19085\ttrain-mlogloss:0.15154                              \n",
      "[44]\teval-mlogloss:0.18281\ttrain-mlogloss:0.14300                              \n",
      "[45]\teval-mlogloss:0.17690\ttrain-mlogloss:0.13706                              \n",
      "[46]\teval-mlogloss:0.16966\ttrain-mlogloss:0.13064                              \n",
      "[47]\teval-mlogloss:0.16403\ttrain-mlogloss:0.12520                              \n",
      "[48]\teval-mlogloss:0.15844\ttrain-mlogloss:0.12015                              \n",
      "[49]\teval-mlogloss:0.15359\ttrain-mlogloss:0.11521                              \n",
      "[50]\teval-mlogloss:0.14927\ttrain-mlogloss:0.11123                              \n",
      "[51]\teval-mlogloss:0.14323\ttrain-mlogloss:0.10603                              \n",
      "[52]\teval-mlogloss:0.13955\ttrain-mlogloss:0.10270                              \n",
      "[53]\teval-mlogloss:0.13539\ttrain-mlogloss:0.09893                              \n",
      "[54]\teval-mlogloss:0.13115\ttrain-mlogloss:0.09516                              \n",
      "[55]\teval-mlogloss:0.12710\ttrain-mlogloss:0.09132                              \n",
      "[56]\teval-mlogloss:0.12359\ttrain-mlogloss:0.08815                              \n",
      "[57]\teval-mlogloss:0.12003\ttrain-mlogloss:0.08505                              \n",
      "[58]\teval-mlogloss:0.11728\ttrain-mlogloss:0.08240                              \n",
      "[59]\teval-mlogloss:0.11297\ttrain-mlogloss:0.07861                              \n",
      "[60]\teval-mlogloss:0.11021\ttrain-mlogloss:0.07629                              \n",
      "[61]\teval-mlogloss:0.10725\ttrain-mlogloss:0.07343                              \n",
      "[62]\teval-mlogloss:0.10499\ttrain-mlogloss:0.07156                              \n",
      "[63]\teval-mlogloss:0.10168\ttrain-mlogloss:0.06889                              \n",
      "[64]\teval-mlogloss:0.09888\ttrain-mlogloss:0.06616                              \n",
      "[65]\teval-mlogloss:0.09603\ttrain-mlogloss:0.06355                              \n",
      "[66]\teval-mlogloss:0.09345\ttrain-mlogloss:0.06132                              \n",
      "[67]\teval-mlogloss:0.09128\ttrain-mlogloss:0.05940                              \n",
      "[68]\teval-mlogloss:0.08907\ttrain-mlogloss:0.05734                              \n",
      "[69]\teval-mlogloss:0.08577\ttrain-mlogloss:0.05461                              \n",
      "[70]\teval-mlogloss:0.08416\ttrain-mlogloss:0.05309                              \n",
      "[71]\teval-mlogloss:0.08218\ttrain-mlogloss:0.05128                              \n",
      "[72]\teval-mlogloss:0.08090\ttrain-mlogloss:0.05014                              \n",
      "[73]\teval-mlogloss:0.07925\ttrain-mlogloss:0.04874                              \n",
      "[74]\teval-mlogloss:0.07790\ttrain-mlogloss:0.04763                              \n",
      "[75]\teval-mlogloss:0.07647\ttrain-mlogloss:0.04641                              \n",
      "[76]\teval-mlogloss:0.07444\ttrain-mlogloss:0.04465                              \n",
      "[77]\teval-mlogloss:0.07296\ttrain-mlogloss:0.04332                              \n",
      "[78]\teval-mlogloss:0.07122\ttrain-mlogloss:0.04190                              \n",
      "[79]\teval-mlogloss:0.07006\ttrain-mlogloss:0.04082                              \n",
      "[80]\teval-mlogloss:0.06835\ttrain-mlogloss:0.03939                              \n",
      "[81]\teval-mlogloss:0.06682\ttrain-mlogloss:0.03810                              \n",
      "[82]\teval-mlogloss:0.06548\ttrain-mlogloss:0.03689                              \n",
      "[83]\teval-mlogloss:0.06437\ttrain-mlogloss:0.03588                              \n",
      "[84]\teval-mlogloss:0.06308\ttrain-mlogloss:0.03478                              \n",
      "[85]\teval-mlogloss:0.06167\ttrain-mlogloss:0.03372                              \n",
      "[86]\teval-mlogloss:0.06057\ttrain-mlogloss:0.03280                              \n",
      "[87]\teval-mlogloss:0.05956\ttrain-mlogloss:0.03196                              \n",
      "[88]\teval-mlogloss:0.05833\ttrain-mlogloss:0.03093                              \n",
      "[89]\teval-mlogloss:0.05745\ttrain-mlogloss:0.03014                              \n",
      "[90]\teval-mlogloss:0.05660\ttrain-mlogloss:0.02947                              \n",
      "[91]\teval-mlogloss:0.05524\ttrain-mlogloss:0.02855                              \n",
      "[92]\teval-mlogloss:0.05432\ttrain-mlogloss:0.02779                              \n",
      "[93]\teval-mlogloss:0.05351\ttrain-mlogloss:0.02720                              \n",
      "[94]\teval-mlogloss:0.05243\ttrain-mlogloss:0.02639                              \n",
      "[95]\teval-mlogloss:0.05160\ttrain-mlogloss:0.02568                              \n",
      "[96]\teval-mlogloss:0.05063\ttrain-mlogloss:0.02496                              \n",
      "[97]\teval-mlogloss:0.05034\ttrain-mlogloss:0.02471                              \n",
      "[98]\teval-mlogloss:0.04985\ttrain-mlogloss:0.02429                              \n",
      "[99]\teval-mlogloss:0.04952\ttrain-mlogloss:0.02402                              \n",
      "[100]\teval-mlogloss:0.04930\ttrain-mlogloss:0.02375                             \n",
      "[101]\teval-mlogloss:0.04889\ttrain-mlogloss:0.02337                             \n",
      "[102]\teval-mlogloss:0.04821\ttrain-mlogloss:0.02297                             \n",
      "[103]\teval-mlogloss:0.04780\ttrain-mlogloss:0.02259                             \n",
      "[104]\teval-mlogloss:0.04729\ttrain-mlogloss:0.02216                             \n",
      "[105]\teval-mlogloss:0.04684\ttrain-mlogloss:0.02188                             \n",
      "[106]\teval-mlogloss:0.04632\ttrain-mlogloss:0.02144                             \n",
      "[107]\teval-mlogloss:0.04571\ttrain-mlogloss:0.02098                             \n",
      "[108]\teval-mlogloss:0.04536\ttrain-mlogloss:0.02063                             \n",
      "[109]\teval-mlogloss:0.04487\ttrain-mlogloss:0.02024                             \n",
      "[110]\teval-mlogloss:0.04422\ttrain-mlogloss:0.01986                             \n",
      "[111]\teval-mlogloss:0.04392\ttrain-mlogloss:0.01959                             \n",
      "[112]\teval-mlogloss:0.04351\ttrain-mlogloss:0.01930                             \n",
      "[113]\teval-mlogloss:0.04318\ttrain-mlogloss:0.01906                             \n",
      "[114]\teval-mlogloss:0.04278\ttrain-mlogloss:0.01876                             \n",
      "[115]\teval-mlogloss:0.04266\ttrain-mlogloss:0.01863                             \n",
      "[116]\teval-mlogloss:0.04247\ttrain-mlogloss:0.01848                             \n",
      "[117]\teval-mlogloss:0.04230\ttrain-mlogloss:0.01832                             \n",
      "[118]\teval-mlogloss:0.04187\ttrain-mlogloss:0.01799                             \n",
      "[119]\teval-mlogloss:0.04155\ttrain-mlogloss:0.01781                             \n",
      "[120]\teval-mlogloss:0.04135\ttrain-mlogloss:0.01759                             \n",
      "[121]\teval-mlogloss:0.04124\ttrain-mlogloss:0.01749                             \n",
      "[122]\teval-mlogloss:0.04112\ttrain-mlogloss:0.01736                             \n",
      "[123]\teval-mlogloss:0.04060\ttrain-mlogloss:0.01709                             \n",
      "[124]\teval-mlogloss:0.04034\ttrain-mlogloss:0.01688                             \n",
      "[125]\teval-mlogloss:0.04002\ttrain-mlogloss:0.01667                             \n",
      "[126]\teval-mlogloss:0.03981\ttrain-mlogloss:0.01653                             \n",
      "[127]\teval-mlogloss:0.03964\ttrain-mlogloss:0.01639                             \n",
      "[128]\teval-mlogloss:0.03943\ttrain-mlogloss:0.01622                             \n",
      "[129]\teval-mlogloss:0.03903\ttrain-mlogloss:0.01599                             \n",
      "[130]\teval-mlogloss:0.03888\ttrain-mlogloss:0.01584                             \n",
      "[131]\teval-mlogloss:0.03866\ttrain-mlogloss:0.01569                             \n",
      "[132]\teval-mlogloss:0.03853\ttrain-mlogloss:0.01554                             \n",
      "[133]\teval-mlogloss:0.03835\ttrain-mlogloss:0.01546                             \n",
      "[134]\teval-mlogloss:0.03827\ttrain-mlogloss:0.01542                             \n",
      "[135]\teval-mlogloss:0.03806\ttrain-mlogloss:0.01529                             \n",
      "[136]\teval-mlogloss:0.03789\ttrain-mlogloss:0.01519                             \n",
      "Best iteration: 137                                                            \n",
      "\tScore 0.9947300717872111                                                      \n",
      "\n",
      "\n",
      "Training with params:                                                            \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.65, 'eta': 0.225, 'eval_metric': 'mlogloss', 'gamma': 0.6000000000000001, 'max_depth': 10, 'min_child_weight': 5.0, 'n_estimators': 106.0, 'nthread': 4, 'num_class': 9, 'objective': 'multi:softmax', 'seed': 314159265, 'silent': 1, 'subsample': 0.55, 'tree_method': 'exact'}\n",
      " 40%|████      | 2/5 [02:32<04:13, 84.63s/trial, best loss: 0.005269928212788932]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jan67814\\AppData\\Local\\miniforge3\\envs\\env_dl_ml\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:37:26] WARNING: D:\\bld\\xgboost-split_1738395203399\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:1.60265\ttrain-mlogloss:1.57353                                 \n",
      "[1]\teval-mlogloss:1.29391\ttrain-mlogloss:1.25246                                 \n",
      "[2]\teval-mlogloss:1.06870\ttrain-mlogloss:1.01882                                 \n",
      "[3]\teval-mlogloss:0.90362\ttrain-mlogloss:0.84986                                 \n",
      "[4]\teval-mlogloss:0.77891\ttrain-mlogloss:0.72272                                 \n",
      "[5]\teval-mlogloss:0.67573\ttrain-mlogloss:0.61853                                 \n",
      "[6]\teval-mlogloss:0.58837\ttrain-mlogloss:0.52970                                 \n",
      "[7]\teval-mlogloss:0.51806\ttrain-mlogloss:0.45900                                 \n",
      "[8]\teval-mlogloss:0.45784\ttrain-mlogloss:0.39916                                 \n",
      "[9]\teval-mlogloss:0.40952\ttrain-mlogloss:0.35223                                 \n",
      "[10]\teval-mlogloss:0.36822\ttrain-mlogloss:0.31131                                \n",
      "[11]\teval-mlogloss:0.33087\ttrain-mlogloss:0.27540                                \n",
      "[12]\teval-mlogloss:0.30098\ttrain-mlogloss:0.24650                                \n",
      "[13]\teval-mlogloss:0.27431\ttrain-mlogloss:0.22102                                \n",
      "[14]\teval-mlogloss:0.25269\ttrain-mlogloss:0.20082                                \n",
      "[15]\teval-mlogloss:0.23265\ttrain-mlogloss:0.18236                                \n",
      "[16]\teval-mlogloss:0.21544\ttrain-mlogloss:0.16637                                \n",
      "[17]\teval-mlogloss:0.20105\ttrain-mlogloss:0.15321                                \n",
      "[18]\teval-mlogloss:0.18755\ttrain-mlogloss:0.14103                                \n",
      "[19]\teval-mlogloss:0.17523\ttrain-mlogloss:0.12972                                \n",
      "[20]\teval-mlogloss:0.16373\ttrain-mlogloss:0.11923                                \n",
      "[21]\teval-mlogloss:0.15296\ttrain-mlogloss:0.10938                                \n",
      "[22]\teval-mlogloss:0.14472\ttrain-mlogloss:0.10198                                \n",
      "[23]\teval-mlogloss:0.13592\ttrain-mlogloss:0.09422                                \n",
      "[24]\teval-mlogloss:0.12807\ttrain-mlogloss:0.08716                                \n",
      "[25]\teval-mlogloss:0.12150\ttrain-mlogloss:0.08157                                \n",
      "[26]\teval-mlogloss:0.11511\ttrain-mlogloss:0.07588                                \n",
      "[27]\teval-mlogloss:0.10959\ttrain-mlogloss:0.07100                                \n",
      "[28]\teval-mlogloss:0.10387\ttrain-mlogloss:0.06595                                \n",
      "[29]\teval-mlogloss:0.09937\ttrain-mlogloss:0.06193                                \n",
      "[30]\teval-mlogloss:0.09485\ttrain-mlogloss:0.05809                                \n",
      "[31]\teval-mlogloss:0.09146\ttrain-mlogloss:0.05534                                \n",
      "[32]\teval-mlogloss:0.08750\ttrain-mlogloss:0.05223                                \n",
      "[33]\teval-mlogloss:0.08325\ttrain-mlogloss:0.04865                                \n",
      "[34]\teval-mlogloss:0.07994\ttrain-mlogloss:0.04597                                \n",
      "[35]\teval-mlogloss:0.07630\ttrain-mlogloss:0.04315                                \n",
      "[36]\teval-mlogloss:0.07440\ttrain-mlogloss:0.04158                                \n",
      "[37]\teval-mlogloss:0.07092\ttrain-mlogloss:0.03898                                \n",
      "[38]\teval-mlogloss:0.06873\ttrain-mlogloss:0.03728                                \n",
      "[39]\teval-mlogloss:0.06644\ttrain-mlogloss:0.03541                                \n",
      "[40]\teval-mlogloss:0.06361\ttrain-mlogloss:0.03337                                \n",
      "[41]\teval-mlogloss:0.06164\ttrain-mlogloss:0.03192                                \n",
      "[42]\teval-mlogloss:0.05999\ttrain-mlogloss:0.03069                                \n",
      "[43]\teval-mlogloss:0.05820\ttrain-mlogloss:0.02937                                \n",
      "[44]\teval-mlogloss:0.05690\ttrain-mlogloss:0.02830                                \n",
      "[45]\teval-mlogloss:0.05509\ttrain-mlogloss:0.02693                                \n",
      "[46]\teval-mlogloss:0.05338\ttrain-mlogloss:0.02570                                \n",
      "[47]\teval-mlogloss:0.05190\ttrain-mlogloss:0.02459                                \n",
      "[48]\teval-mlogloss:0.05075\ttrain-mlogloss:0.02371                                \n",
      "[49]\teval-mlogloss:0.04988\ttrain-mlogloss:0.02304                                \n",
      "[50]\teval-mlogloss:0.04892\ttrain-mlogloss:0.02235                                \n",
      "[51]\teval-mlogloss:0.04797\ttrain-mlogloss:0.02163                                \n",
      "[52]\teval-mlogloss:0.04704\ttrain-mlogloss:0.02103                                \n",
      "[53]\teval-mlogloss:0.04609\ttrain-mlogloss:0.02040                                \n",
      "[54]\teval-mlogloss:0.04521\ttrain-mlogloss:0.01974                                \n",
      "[55]\teval-mlogloss:0.04459\ttrain-mlogloss:0.01922                                \n",
      "[56]\teval-mlogloss:0.04399\ttrain-mlogloss:0.01885                                \n",
      "[57]\teval-mlogloss:0.04342\ttrain-mlogloss:0.01842                                \n",
      "[58]\teval-mlogloss:0.04293\ttrain-mlogloss:0.01812                                \n",
      "[59]\teval-mlogloss:0.04253\ttrain-mlogloss:0.01777                                \n",
      "[60]\teval-mlogloss:0.04198\ttrain-mlogloss:0.01738                                \n",
      "[61]\teval-mlogloss:0.04139\ttrain-mlogloss:0.01697                                \n",
      "[62]\teval-mlogloss:0.04085\ttrain-mlogloss:0.01656                                \n",
      "[63]\teval-mlogloss:0.04033\ttrain-mlogloss:0.01618                                \n",
      "[64]\teval-mlogloss:0.04006\ttrain-mlogloss:0.01594                                \n",
      "[65]\teval-mlogloss:0.03972\ttrain-mlogloss:0.01563                                \n",
      "[66]\teval-mlogloss:0.03929\ttrain-mlogloss:0.01540                                \n",
      "[67]\teval-mlogloss:0.03903\ttrain-mlogloss:0.01521                                \n",
      "[68]\teval-mlogloss:0.03873\ttrain-mlogloss:0.01500                                \n",
      "[69]\teval-mlogloss:0.03823\ttrain-mlogloss:0.01471                                \n",
      "[70]\teval-mlogloss:0.03787\ttrain-mlogloss:0.01447                                \n",
      "[71]\teval-mlogloss:0.03743\ttrain-mlogloss:0.01423                                \n",
      "[72]\teval-mlogloss:0.03724\ttrain-mlogloss:0.01408                                \n",
      "[73]\teval-mlogloss:0.03711\ttrain-mlogloss:0.01394                                \n",
      "[74]\teval-mlogloss:0.03671\ttrain-mlogloss:0.01373                                \n",
      "[75]\teval-mlogloss:0.03655\ttrain-mlogloss:0.01362                                \n",
      "[76]\teval-mlogloss:0.03631\ttrain-mlogloss:0.01344                                \n",
      "[77]\teval-mlogloss:0.03606\ttrain-mlogloss:0.01331                                \n",
      "[78]\teval-mlogloss:0.03605\ttrain-mlogloss:0.01325                                \n",
      "[79]\teval-mlogloss:0.03589\ttrain-mlogloss:0.01318                                \n",
      "[80]\teval-mlogloss:0.03576\ttrain-mlogloss:0.01308                                \n",
      "[81]\teval-mlogloss:0.03563\ttrain-mlogloss:0.01299                                \n",
      "[82]\teval-mlogloss:0.03553\ttrain-mlogloss:0.01284                                \n",
      "[83]\teval-mlogloss:0.03547\ttrain-mlogloss:0.01279                                \n",
      "[84]\teval-mlogloss:0.03522\ttrain-mlogloss:0.01264                                \n",
      "[85]\teval-mlogloss:0.03497\ttrain-mlogloss:0.01253                                \n",
      "[86]\teval-mlogloss:0.03474\ttrain-mlogloss:0.01242                                \n",
      "[87]\teval-mlogloss:0.03471\ttrain-mlogloss:0.01238                                \n",
      "[88]\teval-mlogloss:0.03463\ttrain-mlogloss:0.01230                                \n",
      "[89]\teval-mlogloss:0.03446\ttrain-mlogloss:0.01219                                \n",
      "[90]\teval-mlogloss:0.03427\ttrain-mlogloss:0.01212                                \n",
      "[91]\teval-mlogloss:0.03413\ttrain-mlogloss:0.01205                                \n",
      "[92]\teval-mlogloss:0.03395\ttrain-mlogloss:0.01195                                \n",
      "[93]\teval-mlogloss:0.03389\ttrain-mlogloss:0.01189                                \n",
      "[94]\teval-mlogloss:0.03379\ttrain-mlogloss:0.01180                                \n",
      "[95]\teval-mlogloss:0.03365\ttrain-mlogloss:0.01173                                \n",
      "[96]\teval-mlogloss:0.03356\ttrain-mlogloss:0.01167                                \n",
      "[97]\teval-mlogloss:0.03345\ttrain-mlogloss:0.01161                                \n",
      "[98]\teval-mlogloss:0.03344\ttrain-mlogloss:0.01155                                \n",
      "[99]\teval-mlogloss:0.03344\ttrain-mlogloss:0.01152                                \n",
      "[100]\teval-mlogloss:0.03339\ttrain-mlogloss:0.01148                               \n",
      "[101]\teval-mlogloss:0.03334\ttrain-mlogloss:0.01145                               \n",
      "[102]\teval-mlogloss:0.03328\ttrain-mlogloss:0.01140                               \n",
      "[103]\teval-mlogloss:0.03323\ttrain-mlogloss:0.01137                               \n",
      "[104]\teval-mlogloss:0.03313\ttrain-mlogloss:0.01130                               \n",
      "[105]\teval-mlogloss:0.03306\ttrain-mlogloss:0.01126                               \n",
      "Best iteration: 106                                                              \n",
      "\tScore 0.9955184862442426                                                        \n",
      "\n",
      "\n",
      "Training with params:                                                             \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.8, 'eta': 0.45, 'eval_metric': 'mlogloss', 'gamma': 0.75, 'max_depth': 2, 'min_child_weight': 1.0, 'n_estimators': 112.0, 'nthread': 4, 'num_class': 9, 'objective': 'multi:softmax', 'seed': 314159265, 'silent': 1, 'subsample': 0.9500000000000001, 'tree_method': 'exact'}\n",
      " 60%|██████    | 3/5 [04:42<03:30, 105.48s/trial, best loss: 0.004481513755757449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jan67814\\AppData\\Local\\miniforge3\\envs\\env_dl_ml\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:39:36] WARNING: D:\\bld\\xgboost-split_1738395203399\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:1.73513\ttrain-mlogloss:1.72692                                  \n",
      "[1]\teval-mlogloss:1.55372\ttrain-mlogloss:1.53245                                  \n",
      "[2]\teval-mlogloss:1.42851\ttrain-mlogloss:1.40186                                  \n",
      "[3]\teval-mlogloss:1.34232\ttrain-mlogloss:1.30933                                  \n",
      "[4]\teval-mlogloss:1.27912\ttrain-mlogloss:1.24345                                  \n",
      "[5]\teval-mlogloss:1.22944\ttrain-mlogloss:1.19157                                  \n",
      "[6]\teval-mlogloss:1.18511\ttrain-mlogloss:1.14705                                  \n",
      "[7]\teval-mlogloss:1.14997\ttrain-mlogloss:1.11244                                  \n",
      "[8]\teval-mlogloss:1.11988\ttrain-mlogloss:1.08388                                  \n",
      "[9]\teval-mlogloss:1.09456\ttrain-mlogloss:1.05817                                  \n",
      "[10]\teval-mlogloss:1.07097\ttrain-mlogloss:1.03483                                 \n",
      "[11]\teval-mlogloss:1.04994\ttrain-mlogloss:1.01324                                 \n",
      "[12]\teval-mlogloss:1.02876\ttrain-mlogloss:0.99454                                 \n",
      "[13]\teval-mlogloss:1.01289\ttrain-mlogloss:0.97966                                 \n",
      "[14]\teval-mlogloss:0.99725\ttrain-mlogloss:0.96461                                 \n",
      "[15]\teval-mlogloss:0.98480\ttrain-mlogloss:0.95000                                 \n",
      "[16]\teval-mlogloss:0.96975\ttrain-mlogloss:0.93658                                 \n",
      "[17]\teval-mlogloss:0.95699\ttrain-mlogloss:0.92358                                 \n",
      "[18]\teval-mlogloss:0.94567\ttrain-mlogloss:0.91246                                 \n",
      "[19]\teval-mlogloss:0.93522\ttrain-mlogloss:0.90243                                 \n",
      "[20]\teval-mlogloss:0.92546\ttrain-mlogloss:0.89205                                 \n",
      "[21]\teval-mlogloss:0.91467\ttrain-mlogloss:0.88099                                 \n",
      "[22]\teval-mlogloss:0.90503\ttrain-mlogloss:0.87128                                 \n",
      "[23]\teval-mlogloss:0.89660\ttrain-mlogloss:0.86211                                 \n",
      "[24]\teval-mlogloss:0.88803\ttrain-mlogloss:0.85410                                 \n",
      "[25]\teval-mlogloss:0.87910\ttrain-mlogloss:0.84531                                 \n",
      "[26]\teval-mlogloss:0.87036\ttrain-mlogloss:0.83574                                 \n",
      "[27]\teval-mlogloss:0.86249\ttrain-mlogloss:0.82834                                 \n",
      "[28]\teval-mlogloss:0.85621\ttrain-mlogloss:0.82218                                 \n",
      "[29]\teval-mlogloss:0.84919\ttrain-mlogloss:0.81574                                 \n",
      "[30]\teval-mlogloss:0.84186\ttrain-mlogloss:0.80815                                 \n",
      "[31]\teval-mlogloss:0.83474\ttrain-mlogloss:0.80050                                 \n",
      "[32]\teval-mlogloss:0.82770\ttrain-mlogloss:0.79441                                 \n",
      "[33]\teval-mlogloss:0.82142\ttrain-mlogloss:0.78837                                 \n",
      "[34]\teval-mlogloss:0.81612\ttrain-mlogloss:0.78260                                 \n",
      "[35]\teval-mlogloss:0.80909\ttrain-mlogloss:0.77563                                 \n",
      "[36]\teval-mlogloss:0.80314\ttrain-mlogloss:0.76969                                 \n",
      "[37]\teval-mlogloss:0.79781\ttrain-mlogloss:0.76424                                 \n",
      "[38]\teval-mlogloss:0.79022\ttrain-mlogloss:0.75648                                 \n",
      "[39]\teval-mlogloss:0.78548\ttrain-mlogloss:0.75139                                 \n",
      "[40]\teval-mlogloss:0.78002\ttrain-mlogloss:0.74491                                 \n",
      "[41]\teval-mlogloss:0.77526\ttrain-mlogloss:0.73965                                 \n",
      "[42]\teval-mlogloss:0.76988\ttrain-mlogloss:0.73448                                 \n",
      "[43]\teval-mlogloss:0.76598\ttrain-mlogloss:0.73094                                 \n",
      "[44]\teval-mlogloss:0.75987\ttrain-mlogloss:0.72542                                 \n",
      "[45]\teval-mlogloss:0.75543\ttrain-mlogloss:0.72090                                 \n",
      "[46]\teval-mlogloss:0.75135\ttrain-mlogloss:0.71538                                 \n",
      "[47]\teval-mlogloss:0.74558\ttrain-mlogloss:0.70951                                 \n",
      "[48]\teval-mlogloss:0.73975\ttrain-mlogloss:0.70367                                 \n",
      "[49]\teval-mlogloss:0.73473\ttrain-mlogloss:0.69903                                 \n",
      "[50]\teval-mlogloss:0.72996\ttrain-mlogloss:0.69455                                 \n",
      "[51]\teval-mlogloss:0.72505\ttrain-mlogloss:0.68890                                 \n",
      "[52]\teval-mlogloss:0.72138\ttrain-mlogloss:0.68470                                 \n",
      "[53]\teval-mlogloss:0.71670\ttrain-mlogloss:0.67971                                 \n",
      "[54]\teval-mlogloss:0.71253\ttrain-mlogloss:0.67517                                 \n",
      "[55]\teval-mlogloss:0.70850\ttrain-mlogloss:0.67075                                 \n",
      "[56]\teval-mlogloss:0.70381\ttrain-mlogloss:0.66580                                 \n",
      "[57]\teval-mlogloss:0.69979\ttrain-mlogloss:0.66173                                 \n",
      "[58]\teval-mlogloss:0.69632\ttrain-mlogloss:0.65734                                 \n",
      "[59]\teval-mlogloss:0.69243\ttrain-mlogloss:0.65292                                 \n",
      "[60]\teval-mlogloss:0.68880\ttrain-mlogloss:0.64924                                 \n",
      "[61]\teval-mlogloss:0.68578\ttrain-mlogloss:0.64608                                 \n",
      "[62]\teval-mlogloss:0.68219\ttrain-mlogloss:0.64214                                 \n",
      "[63]\teval-mlogloss:0.67845\ttrain-mlogloss:0.63860                                 \n",
      "[64]\teval-mlogloss:0.67403\ttrain-mlogloss:0.63419                                 \n",
      "[65]\teval-mlogloss:0.67012\ttrain-mlogloss:0.63022                                 \n",
      "[66]\teval-mlogloss:0.66634\ttrain-mlogloss:0.62647                                 \n",
      "[67]\teval-mlogloss:0.66349\ttrain-mlogloss:0.62353                                 \n",
      "[68]\teval-mlogloss:0.65926\ttrain-mlogloss:0.61959                                 \n",
      "[69]\teval-mlogloss:0.65553\ttrain-mlogloss:0.61624                                 \n",
      "[70]\teval-mlogloss:0.65148\ttrain-mlogloss:0.61263                                 \n",
      "[71]\teval-mlogloss:0.64910\ttrain-mlogloss:0.60999                                 \n",
      "[72]\teval-mlogloss:0.64582\ttrain-mlogloss:0.60680                                 \n",
      "[73]\teval-mlogloss:0.64249\ttrain-mlogloss:0.60363                                 \n",
      "[74]\teval-mlogloss:0.63838\ttrain-mlogloss:0.59988                                 \n",
      "[75]\teval-mlogloss:0.63412\ttrain-mlogloss:0.59596                                 \n",
      "[76]\teval-mlogloss:0.63060\ttrain-mlogloss:0.59279                                 \n",
      "[77]\teval-mlogloss:0.62794\ttrain-mlogloss:0.58976                                 \n",
      "[78]\teval-mlogloss:0.62422\ttrain-mlogloss:0.58633                                 \n",
      "[79]\teval-mlogloss:0.62127\ttrain-mlogloss:0.58318                                 \n",
      "[80]\teval-mlogloss:0.61942\ttrain-mlogloss:0.58063                                 \n",
      "[81]\teval-mlogloss:0.61730\ttrain-mlogloss:0.57822                                 \n",
      "[82]\teval-mlogloss:0.61397\ttrain-mlogloss:0.57514                                 \n",
      "[83]\teval-mlogloss:0.61063\ttrain-mlogloss:0.57153                                 \n",
      "[84]\teval-mlogloss:0.60817\ttrain-mlogloss:0.56883                                 \n",
      "[85]\teval-mlogloss:0.60555\ttrain-mlogloss:0.56608                                 \n",
      "[86]\teval-mlogloss:0.60252\ttrain-mlogloss:0.56314                                 \n",
      "[87]\teval-mlogloss:0.59994\ttrain-mlogloss:0.56027                                 \n",
      "[88]\teval-mlogloss:0.59700\ttrain-mlogloss:0.55701                                 \n",
      "[89]\teval-mlogloss:0.59404\ttrain-mlogloss:0.55378                                 \n",
      "[90]\teval-mlogloss:0.59137\ttrain-mlogloss:0.55059                                 \n",
      "[91]\teval-mlogloss:0.58718\ttrain-mlogloss:0.54717                                 \n",
      "[92]\teval-mlogloss:0.58431\ttrain-mlogloss:0.54422                                 \n",
      "[93]\teval-mlogloss:0.58275\ttrain-mlogloss:0.54246                                 \n",
      "[94]\teval-mlogloss:0.58068\ttrain-mlogloss:0.54052                                 \n",
      "[95]\teval-mlogloss:0.57705\ttrain-mlogloss:0.53737                                 \n",
      "[96]\teval-mlogloss:0.57432\ttrain-mlogloss:0.53478                                 \n",
      "[97]\teval-mlogloss:0.57058\ttrain-mlogloss:0.53159                                 \n",
      "[98]\teval-mlogloss:0.56882\ttrain-mlogloss:0.52958                                 \n",
      "[99]\teval-mlogloss:0.56632\ttrain-mlogloss:0.52698                                 \n",
      "[100]\teval-mlogloss:0.56394\ttrain-mlogloss:0.52421                                \n",
      "[101]\teval-mlogloss:0.56126\ttrain-mlogloss:0.52157                                \n",
      "[102]\teval-mlogloss:0.55879\ttrain-mlogloss:0.51907                                \n",
      "[103]\teval-mlogloss:0.55668\ttrain-mlogloss:0.51646                                \n",
      "[104]\teval-mlogloss:0.55418\ttrain-mlogloss:0.51390                                \n",
      "[105]\teval-mlogloss:0.55142\ttrain-mlogloss:0.51120                                \n",
      "[106]\teval-mlogloss:0.54953\ttrain-mlogloss:0.50912                                \n",
      "[107]\teval-mlogloss:0.54827\ttrain-mlogloss:0.50735                                \n",
      "[108]\teval-mlogloss:0.54550\ttrain-mlogloss:0.50468                                \n",
      "[109]\teval-mlogloss:0.54309\ttrain-mlogloss:0.50235                                \n",
      "[110]\teval-mlogloss:0.54143\ttrain-mlogloss:0.50056                                \n",
      "[111]\teval-mlogloss:0.53874\ttrain-mlogloss:0.49793                                \n",
      "Best iteration: 112                                                               \n",
      "\tScore 0.8177932694302669                                                         \n",
      "\n",
      "\n",
      "Training with params:                                                             \n",
      "{'booster': 'gbtree', 'colsample_bytree': 0.9500000000000001, 'eta': 0.275, 'eval_metric': 'mlogloss', 'gamma': 0.9, 'max_depth': 8, 'min_child_weight': 5.0, 'n_estimators': 136.0, 'nthread': 4, 'num_class': 9, 'objective': 'multi:softmax', 'seed': 314159265, 'silent': 1, 'subsample': 0.8, 'tree_method': 'exact'}\n",
      " 80%|████████  | 4/5 [05:24<01:20, 80.56s/trial, best loss: 0.004481513755757449]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jan67814\\AppData\\Local\\miniforge3\\envs\\env_dl_ml\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:40:18] WARNING: D:\\bld\\xgboost-split_1738395203399\\work\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\teval-mlogloss:1.51758\ttrain-mlogloss:1.49303                                 \n",
      "[1]\teval-mlogloss:1.21468\ttrain-mlogloss:1.17873                                 \n",
      "[2]\teval-mlogloss:1.00941\ttrain-mlogloss:0.96655                                 \n",
      "[3]\teval-mlogloss:0.85791\ttrain-mlogloss:0.81151                                 \n",
      "[4]\teval-mlogloss:0.74405\ttrain-mlogloss:0.69545                                 \n",
      "[5]\teval-mlogloss:0.64906\ttrain-mlogloss:0.59925                                 \n",
      "[6]\teval-mlogloss:0.57379\ttrain-mlogloss:0.52398                                 \n",
      "[7]\teval-mlogloss:0.51436\ttrain-mlogloss:0.46511                                 \n",
      "[8]\teval-mlogloss:0.46688\ttrain-mlogloss:0.41822                                 \n",
      "[9]\teval-mlogloss:0.42707\ttrain-mlogloss:0.37923                                 \n",
      "[10]\teval-mlogloss:0.39612\ttrain-mlogloss:0.34839                                \n",
      "[11]\teval-mlogloss:0.36837\ttrain-mlogloss:0.32040                                \n",
      "[12]\teval-mlogloss:0.34162\ttrain-mlogloss:0.29446                                \n",
      "[13]\teval-mlogloss:0.32055\ttrain-mlogloss:0.27413                                \n",
      "[14]\teval-mlogloss:0.29963\ttrain-mlogloss:0.25397                                \n",
      "[15]\teval-mlogloss:0.27995\ttrain-mlogloss:0.23447                                \n",
      "[16]\teval-mlogloss:0.26403\ttrain-mlogloss:0.21991                                \n",
      "[17]\teval-mlogloss:0.24646\ttrain-mlogloss:0.20227                                \n",
      "[18]\teval-mlogloss:0.23437\ttrain-mlogloss:0.19078                                \n",
      "[19]\teval-mlogloss:0.21914\ttrain-mlogloss:0.17606                                \n",
      "[20]\teval-mlogloss:0.20748\ttrain-mlogloss:0.16471                                \n",
      "[21]\teval-mlogloss:0.19630\ttrain-mlogloss:0.15358                                \n",
      "[22]\teval-mlogloss:0.18683\ttrain-mlogloss:0.14437                                \n",
      "[23]\teval-mlogloss:0.17686\ttrain-mlogloss:0.13492                                \n",
      "[24]\teval-mlogloss:0.16897\ttrain-mlogloss:0.12802                                \n",
      "[25]\teval-mlogloss:0.15965\ttrain-mlogloss:0.11942                                \n",
      "[26]\teval-mlogloss:0.15256\ttrain-mlogloss:0.11251                                \n",
      "[27]\teval-mlogloss:0.14703\ttrain-mlogloss:0.10724                                \n",
      "[28]\teval-mlogloss:0.13925\ttrain-mlogloss:0.10020                                \n",
      "[29]\teval-mlogloss:0.13439\ttrain-mlogloss:0.09562                                \n",
      "[30]\teval-mlogloss:0.12776\ttrain-mlogloss:0.08964                                \n",
      "[31]\teval-mlogloss:0.12297\ttrain-mlogloss:0.08530                                \n",
      "[32]\teval-mlogloss:0.11763\ttrain-mlogloss:0.08035                                \n",
      "[33]\teval-mlogloss:0.11135\ttrain-mlogloss:0.07456                                \n",
      "[34]\teval-mlogloss:0.10650\ttrain-mlogloss:0.07050                                \n",
      "[35]\teval-mlogloss:0.10124\ttrain-mlogloss:0.06579                                \n",
      "[36]\teval-mlogloss:0.09766\ttrain-mlogloss:0.06234                                \n",
      "[37]\teval-mlogloss:0.09320\ttrain-mlogloss:0.05814                                \n",
      "[38]\teval-mlogloss:0.09002\ttrain-mlogloss:0.05563                                \n",
      "[39]\teval-mlogloss:0.08637\ttrain-mlogloss:0.05233                                \n",
      "[40]\teval-mlogloss:0.08258\ttrain-mlogloss:0.04899                                \n",
      "[41]\teval-mlogloss:0.07913\ttrain-mlogloss:0.04606                                \n",
      "[42]\teval-mlogloss:0.07601\ttrain-mlogloss:0.04349                                \n",
      "[43]\teval-mlogloss:0.07320\ttrain-mlogloss:0.04118                                \n",
      "[44]\teval-mlogloss:0.07011\ttrain-mlogloss:0.03850                                \n",
      "[45]\teval-mlogloss:0.06776\ttrain-mlogloss:0.03645                                \n",
      "[46]\teval-mlogloss:0.06670\ttrain-mlogloss:0.03548                                \n",
      "[47]\teval-mlogloss:0.06410\ttrain-mlogloss:0.03332                                \n",
      "[48]\teval-mlogloss:0.06160\ttrain-mlogloss:0.03141                                \n",
      "[49]\teval-mlogloss:0.06025\ttrain-mlogloss:0.03020                                \n",
      "[50]\teval-mlogloss:0.05893\ttrain-mlogloss:0.02905                                \n",
      "[51]\teval-mlogloss:0.05737\ttrain-mlogloss:0.02775                                \n",
      "[52]\teval-mlogloss:0.05617\ttrain-mlogloss:0.02684                                \n",
      "[53]\teval-mlogloss:0.05505\ttrain-mlogloss:0.02585                                \n",
      "[54]\teval-mlogloss:0.05377\ttrain-mlogloss:0.02488                                \n",
      "[55]\teval-mlogloss:0.05233\ttrain-mlogloss:0.02382                                \n",
      "[56]\teval-mlogloss:0.05112\ttrain-mlogloss:0.02288                                \n",
      "[57]\teval-mlogloss:0.04945\ttrain-mlogloss:0.02170                                \n",
      "[58]\teval-mlogloss:0.04840\ttrain-mlogloss:0.02095                                \n",
      "[59]\teval-mlogloss:0.04765\ttrain-mlogloss:0.02034                                \n",
      "[60]\teval-mlogloss:0.04644\ttrain-mlogloss:0.01949                                \n",
      "[61]\teval-mlogloss:0.04550\ttrain-mlogloss:0.01892                                \n",
      "[62]\teval-mlogloss:0.04487\ttrain-mlogloss:0.01833                                \n",
      "[63]\teval-mlogloss:0.04405\ttrain-mlogloss:0.01784                                \n",
      "[64]\teval-mlogloss:0.04356\ttrain-mlogloss:0.01743                                \n",
      "[65]\teval-mlogloss:0.04309\ttrain-mlogloss:0.01707                                \n",
      "[66]\teval-mlogloss:0.04239\ttrain-mlogloss:0.01659                                \n",
      "[67]\teval-mlogloss:0.04183\ttrain-mlogloss:0.01611                                \n",
      "[68]\teval-mlogloss:0.04131\ttrain-mlogloss:0.01577                                \n",
      "[69]\teval-mlogloss:0.04103\ttrain-mlogloss:0.01556                                \n",
      "[70]\teval-mlogloss:0.04037\ttrain-mlogloss:0.01516                                \n",
      "[71]\teval-mlogloss:0.04014\ttrain-mlogloss:0.01493                                \n",
      "[72]\teval-mlogloss:0.03967\ttrain-mlogloss:0.01462                                \n",
      "[73]\teval-mlogloss:0.03941\ttrain-mlogloss:0.01449                                \n",
      "[74]\teval-mlogloss:0.03914\ttrain-mlogloss:0.01422                                \n",
      "[75]\teval-mlogloss:0.03894\ttrain-mlogloss:0.01405                                \n",
      "[76]\teval-mlogloss:0.03882\ttrain-mlogloss:0.01394                                \n",
      "[77]\teval-mlogloss:0.03865\ttrain-mlogloss:0.01384                                \n",
      "[78]\teval-mlogloss:0.03835\ttrain-mlogloss:0.01360                                \n",
      "[79]\teval-mlogloss:0.03821\ttrain-mlogloss:0.01348                                \n",
      "[80]\teval-mlogloss:0.03791\ttrain-mlogloss:0.01330                                \n",
      "[81]\teval-mlogloss:0.03765\ttrain-mlogloss:0.01313                                \n",
      "[82]\teval-mlogloss:0.03731\ttrain-mlogloss:0.01296                                \n",
      "[83]\teval-mlogloss:0.03704\ttrain-mlogloss:0.01278                                \n",
      "[84]\teval-mlogloss:0.03664\ttrain-mlogloss:0.01257                                \n",
      "[85]\teval-mlogloss:0.03651\ttrain-mlogloss:0.01251                                \n",
      "[86]\teval-mlogloss:0.03637\ttrain-mlogloss:0.01241                                \n",
      "[87]\teval-mlogloss:0.03633\ttrain-mlogloss:0.01234                                \n",
      "[88]\teval-mlogloss:0.03625\ttrain-mlogloss:0.01225                                \n",
      "[89]\teval-mlogloss:0.03624\ttrain-mlogloss:0.01220                                \n",
      "[90]\teval-mlogloss:0.03614\ttrain-mlogloss:0.01211                                \n",
      "[91]\teval-mlogloss:0.03600\ttrain-mlogloss:0.01202                                \n",
      "[92]\teval-mlogloss:0.03585\ttrain-mlogloss:0.01190                                \n",
      "[93]\teval-mlogloss:0.03570\ttrain-mlogloss:0.01181                                \n",
      "[94]\teval-mlogloss:0.03559\ttrain-mlogloss:0.01175                                \n",
      "[95]\teval-mlogloss:0.03552\ttrain-mlogloss:0.01169                                \n",
      "[96]\teval-mlogloss:0.03547\ttrain-mlogloss:0.01166                                \n",
      "[97]\teval-mlogloss:0.03536\ttrain-mlogloss:0.01161                                \n",
      "[98]\teval-mlogloss:0.03524\ttrain-mlogloss:0.01152                                \n",
      "[99]\teval-mlogloss:0.03516\ttrain-mlogloss:0.01148                                \n",
      "[100]\teval-mlogloss:0.03506\ttrain-mlogloss:0.01139                               \n",
      "[101]\teval-mlogloss:0.03504\ttrain-mlogloss:0.01135                               \n",
      "[102]\teval-mlogloss:0.03496\ttrain-mlogloss:0.01126                               \n",
      "[103]\teval-mlogloss:0.03494\ttrain-mlogloss:0.01125                               \n",
      "[104]\teval-mlogloss:0.03484\ttrain-mlogloss:0.01120                               \n",
      "[105]\teval-mlogloss:0.03483\ttrain-mlogloss:0.01117                               \n",
      "[106]\teval-mlogloss:0.03475\ttrain-mlogloss:0.01110                               \n",
      "[107]\teval-mlogloss:0.03467\ttrain-mlogloss:0.01107                               \n",
      "[108]\teval-mlogloss:0.03463\ttrain-mlogloss:0.01102                               \n",
      "[109]\teval-mlogloss:0.03460\ttrain-mlogloss:0.01099                               \n",
      "[110]\teval-mlogloss:0.03459\ttrain-mlogloss:0.01096                               \n",
      "[111]\teval-mlogloss:0.03453\ttrain-mlogloss:0.01089                               \n",
      "[112]\teval-mlogloss:0.03449\ttrain-mlogloss:0.01087                               \n",
      "[113]\teval-mlogloss:0.03437\ttrain-mlogloss:0.01081                               \n",
      "[114]\teval-mlogloss:0.03429\ttrain-mlogloss:0.01078                               \n",
      "[115]\teval-mlogloss:0.03416\ttrain-mlogloss:0.01071                               \n",
      "[116]\teval-mlogloss:0.03409\ttrain-mlogloss:0.01067                               \n",
      "[117]\teval-mlogloss:0.03405\ttrain-mlogloss:0.01064                               \n",
      "[118]\teval-mlogloss:0.03402\ttrain-mlogloss:0.01063                               \n",
      "[119]\teval-mlogloss:0.03393\ttrain-mlogloss:0.01060                               \n",
      "[120]\teval-mlogloss:0.03390\ttrain-mlogloss:0.01059                               \n",
      "[121]\teval-mlogloss:0.03385\ttrain-mlogloss:0.01057                               \n",
      "[122]\teval-mlogloss:0.03387\ttrain-mlogloss:0.01054                               \n",
      "[123]\teval-mlogloss:0.03386\ttrain-mlogloss:0.01053                               \n",
      "[124]\teval-mlogloss:0.03388\ttrain-mlogloss:0.01052                               \n",
      "[125]\teval-mlogloss:0.03387\ttrain-mlogloss:0.01051                               \n",
      "[126]\teval-mlogloss:0.03382\ttrain-mlogloss:0.01049                               \n",
      "[127]\teval-mlogloss:0.03374\ttrain-mlogloss:0.01043                               \n",
      "[128]\teval-mlogloss:0.03370\ttrain-mlogloss:0.01042                               \n",
      "[129]\teval-mlogloss:0.03361\ttrain-mlogloss:0.01039                               \n",
      "[130]\teval-mlogloss:0.03353\ttrain-mlogloss:0.01036                               \n",
      "[131]\teval-mlogloss:0.03355\ttrain-mlogloss:0.01035                               \n",
      "[132]\teval-mlogloss:0.03357\ttrain-mlogloss:0.01032                               \n",
      "[133]\teval-mlogloss:0.03348\ttrain-mlogloss:0.01030                               \n",
      "[134]\teval-mlogloss:0.03346\ttrain-mlogloss:0.01028                               \n",
      "[135]\teval-mlogloss:0.03343\ttrain-mlogloss:0.01027                               \n",
      "Best iteration: 136                                                              \n",
      "\tScore 0.994896053778165                                                         \n",
      "\n",
      "\n",
      "100%|██████████| 5/5 [08:39<00:00, 103.93s/trial, best loss: 0.004481513755757449]\n",
      "The best hyperparameters are:  \n",
      "\n",
      "{'colsample_bytree': 0.65, 'eta': 0.225, 'gamma': 0.6000000000000001, 'max_depth': 9, 'min_child_weight': 5.0, 'n_estimators': 106.0, 'subsample': 0.55}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams = optimize()\n",
    "print(\"The best hyperparameters are: \", \"\\n\")\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and test a classifier\n",
    "def train_and_test(X_tr, y_tr, X_v, well_v):\n",
    "    \n",
    "    # Feature normalization\n",
    "    scaler = preprocessing.RobustScaler(quantile_range=(25.0, 75.0)).fit(X_tr)\n",
    "    X_tr = scaler.transform(X_tr)\n",
    "    X_v = scaler.transform(X_v)\n",
    "    \n",
    "    # Train classifier\n",
    "    #clf = make_pipeline(make_union(VotingClassifier([(\"est\", ExtraTreesClassifier(criterion=\"gini\", max_features=1.0, n_estimators=500))]), FunctionTransformer(lambda X: X)), XGBClassifier(learning_rate=0.73, max_depth=10, min_child_weight=10, n_estimators=500, subsample=0.27))\n",
    "    #clf =  make_pipeline( KNeighborsClassifier(n_neighbors=5, weights=\"distance\") ) \n",
    "    #clf = make_pipeline(MaxAbsScaler(),make_union(VotingClassifier([(\"est\", RandomForestClassifier(n_estimators=500))]), FunctionTransformer(lambda X: X)),ExtraTreesClassifier(criterion=\"entropy\", max_features=0.0001, n_estimators=500))\n",
    "    # * clf = make_pipeline( make_union(VotingClassifier([(\"est\", BernoulliNB(alpha=60.0, binarize=0.26, fit_prior=True))]), FunctionTransformer(lambda X: X)),RandomForestClassifier(n_estimators=500))\n",
    "    # ** clf = make_pipeline ( XGBClassifier(learning_rate=0.12, max_depth=3, min_child_weight=10, n_estimators=150, seed = 17, colsample_bytree = 0.9) )\n",
    "    clf = make_pipeline ( XGBClassifier(learning_rate=0.15, max_depth=8, min_child_weight=4, n_estimators=148, seed = SEED, colsample_bytree = 0.85, subsample = 0.9 , gamma = 0.75) )\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Test classifier\n",
    "    y_v_hat = clf.predict(X_v)\n",
    "    \n",
    "    # Clean isolated facies for each well\n",
    "    for w in np.unique(well_v):\n",
    "        y_v_hat[well_v==w] = medfilt(y_v_hat[well_v==w], kernel_size=5)\n",
    "    \n",
    "    return y_v_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Load testing data\n",
    "test_data = pd.read_csv('../validation_data_nofacies.csv')\n",
    "\n",
    "# Prepare training data\n",
    "X_tr = X\n",
    "y_tr = y\n",
    "\n",
    "# Augment features\n",
    "X_tr, padded_rows = augment_features(X_tr, well, depth)\n",
    "\n",
    "# Removed padded rows\n",
    "X_tr = np.delete(X_tr, padded_rows, axis=0)\n",
    "y_tr = np.delete(y_tr, padded_rows, axis=0) - 1\n",
    "\n",
    "# Prepare test data\n",
    "well_ts = test_data['Well Name'].values\n",
    "depth_ts = test_data['Depth'].values\n",
    "X_ts = test_data[feature_names].values\n",
    "\n",
    "# Augment features\n",
    "X_ts, padded_rows = augment_features(X_ts, well_ts, depth_ts)\n",
    "\n",
    "# Predict test labels\n",
    "y_ts_hat = train_and_test(X_tr, y_tr, X_ts, well_ts)\n",
    "\n",
    "# Save predicted labels\n",
    "test_data['Facies'] = y_ts_hat + 1\n",
    "test_data.to_csv('Prediction_XXX_Final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
